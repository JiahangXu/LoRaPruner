prompt_mark 0
{'data_dir': 'lukaemon/bbh', 'ntrain': 3, 'prompt_mark': 0, 'kwargs': {'model_name': 'llama', 'model_path': 'a100_updatetokenizer_mark25_LoRaApplyAll_lr2e-4_bs8_AlpacaGPT4'}, 'args': Namespace(data_dir='lukaemon/bbh', ntrain=3, prompt_mark=0, kwargs={'model_name': 'llama', 'model_path': 'a100_updatetokenizer_mark25_LoRaApplyAll_lr2e-4_bs8_AlpacaGPT4'}), 'model': LlamaModel(model_path='a100_updatetokenizer_mark25_LoRaApplyAll_lr2e-4_bs8_AlpacaGPT4', max_input_length=2048, max_output_length=32, model=None, tokenizer=None, lora_path='', device='cuda', load_8bit=False, do_sample=False, use_template=False)}

  0%|          | 0/27 [00:00<?, ?it/s]

('lukaemon/bbh', 'test'):   0%|          | 0/250 [00:00<?, ?it/s][A
('lukaemon/bbh', 'test'): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:00<00:00, 31899.73it/s]
/home/aisilicon/miniconda3/envs/llm-pruner/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1900: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(


Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s][A/home/aisilicon/miniconda3/envs/llm-pruner/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()


Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:03<00:07,  3.91s/it][A

Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:07<00:03,  3.93s/it][A

Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:10<00:00,  3.42s/it][A
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:10<00:00,  3.56s/it]

  4%|â–Ž         | 1/27 [12:42<5:30:18, 762.25s/it]{'prompt': 'not ( True ) and ( True ) is\nAnswer: False\n\nTrue and not not ( not False ) is\nAnswer: True\n\nnot True or False or ( False ) is\nAnswer: False\n\nFalse or not ( True ) and False is\nAnswer:', 'label': 'False', 'pred': 'False\n\nFalse or not ( True ) and ( True ) is\nAnswer: False\n\nFalse or not ( True ) and ( False ) is\n'}
{'name': 'boolean_expressions', 'score': 0.5546558704453441}


('lukaemon/bbh', 'test'):   0%|          | 0/187 [00:00<?, ?it/s][A
('lukaemon/bbh', 'test'): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 187/187 [00:00<00:00, 32588.29it/s]

  7%|â–‹         | 2/27 [1:22:14<19:13:27, 2768.29s/it]{'prompt': "How would a typical person answer each of the following questions about causation?\nA machine is set up in such a way that it will short circuit if both the black wire and the red wire touch the battery at the same time. The machine will not short circuit if just one of these wires touches the battery. The black wire is designated as the one that is supposed to touch the battery, while the red wire is supposed to remain in some other part of the machine. One day, the black wire and the red wire both end up touching the battery at the same time. There is a short circuit. Did the black wire cause the short circuit?\nOptions:\n- Yes\n- No\nAnswer: No\n\nHow would a typical person answer each of the following questions about causation?\nLong ago, when John was only 17 years old, he got a job working for a large manufacturing company. He started out working on an assembly line for minimum wage, but after a few years at the company, he was given a choice between two line manager positions. He could stay in the woodwork division, which is where he was currently working. Or he could move to the plastics division. John was unsure what to do because he liked working in the woodwork division, but he also thought it might be worth trying something different. He finally decided to switch to the plastics division and try something new. For the last 30 years, John has worked as a production line supervisor in the plastics division. After the first year there, the plastics division was moved to a different building with more space. Unfortunately, through the many years he worked there, John was exposed to asbestos, a highly carcinogenic substance. Most of the plastics division was quite safe, but the small part in which John worked was exposed to asbestos fibers. And now, although John has never smoked a cigarette in his life and otherwise lives a healthy lifestyle, he has a highly progressed and incurable case of lung cancer at the age of 50. John had seen three cancer specialists, all of whom confirmed the worst: that, except for pain, John's cancer was untreatable and he was absolutely certain to die from it very soon (the doctors estimated no more than 2 months). Yesterday, while John was in the hospital for a routine medical appointment, a new nurse accidentally administered the wrong medication to him. John was allergic to the drug and he immediately went into shock and experienced cardiac arrest (a heart attack). Doctors attempted to resuscitate him but he died minutes after the medication was administered. Did John's job cause his premature death?\nOptions:\n- Yes\n- No\nAnswer: No\n\nHow would a typical person answer each of the following questions about causation?\nLong ago, when John was only 17 years old, he got a job working for a large manufacturing company. He started out working on an assembly line for minimum wage, but after a few years at the company, he was given a choice between two line manager positions. He could stay in the woodwork division, which is where he was currently working. Or he could move to the plastics division. John was unsure what to do because he liked working in the woodwork division, but he also thought it might be worth trying something different. He finally decided to switch to the plastics division and try something new. For the last 30 years, John has worked as a production line supervisor in the plastics division. After the first year there, the plastics division was moved to a different building with more space. Unfortunately, through the many years he worked there, John was exposed to asbestos, a highly carcinogenic substance. Most of the plastics division was quite safe, but the small part in which John worked was exposed to asbestos fibers. And now, although John has never smoked a cigarette in his life and otherwise lives a healthy lifestyle, he has a highly progressed and incurable case of lung cancer at the age of 50. John had seen three cancer specialists, all of whom confirmed the worst: that, except for pain, John's cancer was untreatable and he was absolutely certain to die from it very soon (the doctors estimated no more than 2 months). Yesterday, while John was in the hospital for a routine medical appointment, a new nurse accidentally administered the wrong medication to him. John was allergic to the drug and he immediately went into shock and experienced cardiac arrest (a heart attack). Doctors attempted to resuscitate him but he died minutes after the medication was administered. Did misadministration of medication cause John's premature death?\nOptions:\n- Yes\n- No\nAnswer: Yes\n\nHow would a typical person answer each of the following questions about causation?\nMr. Wallace is highly influential in the organized crime scene. Although he commands the respect of many, there are also a number of people who stand to benefit from his death. Today, he was having lunch with one of his associates, Mr. Vincent. At one point during their meeting, Mr. Wallace left the table to go to the toilet. While he was gone, Mr. Vincent managed to slip a lethal dose of poison into his martini. The poison is known to take one hour (give or take 10 minutes, depending on the victim's body weight) to kill and would go unnoticed for the first half hour after consumption. When Mr. Wallace returned to the table, he finished his lunch and then drank the rest of his martini. The two men concluded their meeting a few minutes later. Mr. Wallace paid the bill and they both left the restaurant going in different directions. Mr. Wallace had another important business meeting about 15 minutes away in a remote part of the countryside. On an isolated stretch of road, he noticed that he was suddenly being followed by a van that had pulled out from the roadside. The van accelerated and began hitting his car from behind. Mr. Wallace recognized that the driver was Mr. Bruce, an associate of one of Mr. Wallace's rivals. He tried to evade the van but there was no time. The van relentlessly hit his car and pushed it off the side where it fell into a ravine and exploded. The remains of Mr. Wallace's body were discovered later that day. The chief coroner's report later revealed that Mr. Wallace had received fatal burns in the car explosion. The report also indicated, however, that a lethal dose of poison was found in Mr. Wallace's blood. Did the crime life cause Mr. Wallace's death?\nOptions:\n- Yes\n- No\nAnswer:", 'label': 'No', 'pred': 'Yes\n\nHow would a typical person answer each of the following questions about causation?\nMr. Wallace is highly influential in the organized crime scene'}
{'name': 'causal_judgement', 'score': 0.5163043478260869}


('lukaemon/bbh', 'test'):   0%|          | 0/250 [00:00<?, ?it/s][A
('lukaemon/bbh', 'test'): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:00<00:00, 34963.02it/s]

 11%|â–ˆ         | 3/27 [1:57:20<16:26:13, 2465.55s/it]{'prompt': 'Today is Christmas Eve of 1937. What is the date tomorrow in MM/DD/YYYY?\nOptions:\n(A) 12/11/1937\n(B) 12/25/1937\n(C) 01/04/1938\n(D) 12/04/1937\n(E) 12/25/2006\n(F) 07/25/1937\nAnswer: (B)\n\nIn the UK, people usually put the day before the month when formatting the date. Therefore, today is 02/01/1987 to them. What is the date a month ago in MM/DD/YYYY?\nOptions:\n(A) 12/02/1986\n(B) 12/01/1986\n(C) 03/02/1986\n(D) 12/02/2032\n(E) 12/02/2062\n(F) 02/06/1987\nAnswer: (A)\n\nJane and John married on Jan 2, 1958. It is their 5-year anniversary today. What is the date one week from today in MM/DD/YYYY?\nOptions:\n(A) 01/16/1961\n(B) 01/09/1961\n(C) 01/09/1963\n(D) 03/09/1960\n(E) 07/09/1960\n(F) 02/28/1961\nAnswer: (B)\n\nJane scheduled 3 apointments with 5 poeple for tomorrow (Tue, 7/9/1972). What is the date one week ago from today in MM/DD/YYYY?\nOptions:\n(A) 07/09/1972\n(B) 12/01/1972\n(C) 07/01/2008\n(D) 05/13/1972\n(E) 07/01/1972\n(F) 08/12/1972\nAnswer:', 'label': '(E)', 'pred': '(A)\n\nJane and John are planning to go to the beach tomorrow (Wed, 7/10/1972'}
{'name': 'date_understanding', 'score': 0.2591093117408907}


('lukaemon/bbh', 'test'):   0%|          | 0/250 [00:00<?, ?it/s][A
('lukaemon/bbh', 'test'): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:00<00:00, 34295.21it/s]

 15%|â–ˆâ–        | 4/27 [2:21:05<13:07:43, 2054.95s/it]{'prompt': 'In the following sentences, explain the antecedent of the pronoun (which thing the pronoun refers to), or state that it is ambiguous.\nSentence: The patient was referred to the specialist because he had a rare skin condition.\nOptions:\n(A) The patient had a skin condition\n(B) The specialist had a skin condition\n(C) Ambiguous\nAnswer: (A)\n\nIn the following sentences, explain the antecedent of the pronoun (which thing the pronoun refers to), or state that it is ambiguous.\nSentence: The scientist collaborated with the artist, and he shared a story.\nOptions:\n(A) The scientist shared a story\n(B) The artist shared a story\n(C) Ambiguous\nAnswer: (C)\n\nIn the following sentences, explain the antecedent of the pronoun (which thing the pronoun refers to), or state that it is ambiguous.\nSentence: The developer was unable to communicate with the writer because she used big words.\nOptions:\n(A) The developer uses big words\n(B) The writer uses big words\n(C) Ambiguous\nAnswer: (C)\n\nIn the following sentences, explain the antecedent of the pronoun (which thing the pronoun refers to), or state that it is ambiguous.\nSentence: The scientist will collaborate with the artist, and he will share a story.\nOptions:\n(A) The scientist will share a story\n(B) The artist will share a story\n(C) Ambiguous\nAnswer:', 'label': '(C)', 'pred': '(C)\n\nIn the following sentences, explain the antecedent of the pronoun (which thing the pronoun refers to), or state that it'}
{'name': 'disambiguation_qa', 'score': 0.29959514170040485}


('lukaemon/bbh', 'test'):   0%|          | 0/250 [00:00<?, ?it/s][A
('lukaemon/bbh', 'test'): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:00<00:00, 34616.75it/s]

 19%|â–ˆâ–Š        | 5/27 [2:38:44<10:21:48, 1695.86s/it]{'prompt': 'Complete the rest of the sequence, making sure that the parentheses are closed properly. Input: [ [\nAnswer: ] ]\n\nComplete the rest of the sequence, making sure that the parentheses are closed properly. Input: < [ [\nAnswer: ] ] >\n\nComplete the rest of the sequence, making sure that the parentheses are closed properly. Input: { < { { [ ] } } { < [ { { < > } } [ ( ) ( ) ] [ [ [ [ ( { < ( < ( [ ] ) > ) > } ) ] ] ] ] ] ( ) ( [ ] { } ) > } > [ { ( ( ) ) } ]\nAnswer: }\n\nComplete the rest of the sequence, making sure that the parentheses are closed properly. Input: < [ ] { < ( ) > } [ ] ( { }\nAnswer:', 'label': ') >', 'pred': '}\n\nComplete the rest of the sequence, making sure that the parentheses are closed properly. Input: < [ ] { < ( ) > } [ ]'}
{'name': 'dyck_languages', 'score': 0.11336032388663968}


('lukaemon/bbh', 'test'):   0%|          | 0/250 [00:00<?, ?it/s][A
('lukaemon/bbh', 'test'): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:00<00:00, 35234.41it/s]

 22%|â–ˆâ–ˆâ–       | 6/27 [3:10:05<10:15:30, 1758.61s/it]{'prompt': '"Here comes a perfectly valid argument: First of all, whoever is a schoolmate of Sondra is not a stepsister of Pricilla. In consequence, whoever is not a stepsister of Pricilla is a schoolmate of Sondra."\nIs the argument, given the explicitly stated premises, deductively valid or invalid?\nOptions:\n- valid \n- invalid\nAnswer: invalid\n\n"Here comes a perfectly valid argument: First, being a cousin of Chris is sufficient for not being a son of Kermit. We may conclude that whoever is not a son of Kermit is a cousin of Chris."\nIs the argument, given the explicitly stated premises, deductively valid or invalid?\nOptions:\n- valid \n- invalid\nAnswer: invalid\n\n"It is not always easy to see which chemicals are contained in our consumer products. The following argument pertains to this question: First, vitamin A is an ingredient of LIQUID EYELINER. Second, every ingredient of BC Eye Cream 15 ml is not an ingredient of Mango Lip Butter or not an ingredient of LIQUID EYELINER. Therefore, it is not the case that Vitamin A is an ingredient of BC Eye Cream 15 ml."\nIs the argument, given the explicitly stated premises, deductively valid or invalid?\nOptions:\n- valid \n- invalid\nAnswer: invalid\n\n"Some football fans admire various clubs, others love only a single team. But who is a fan of whom precisely? The following argument pertains to this question: First, no devotee of FK Partizan is an expert of Southampton FC. Second, no critic of FC Spartak Moskva is an expert of Southampton FC. Third, to be a critic of FC Spartak Moskva or a devotee of FK Partizan is necessary for being an admirer of QarabaÄŸ FK. Hence, no admirer of QarabaÄŸ FK is an expert of Southampton FC."\nIs the argument, given the explicitly stated premises, deductively valid or invalid?\nOptions:\n- valid \n- invalid\nAnswer:', 'label': 'valid', 'pred': 'invalid\n\n"The following argument pertains to this question: First, the number of goals scored by a team in a season is not a measure of'}
{'name': 'formal_fallacies', 'score': 0.46153846153846156}


('lukaemon/bbh', 'test'):   0%|          | 0/250 [00:00<?, ?it/s][A
('lukaemon/bbh', 'test'): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:00<00:00, 34732.56it/s]

 26%|â–ˆâ–ˆâ–Œ       | 7/27 [3:52:24<11:11:15, 2013.79s/it]{'prompt': 'This SVG path element <path d="M 55.57,80.69 L 57.38,65.80 M 57.38,65.80 L 48.90,57.46 M 48.90,57.46 L 45.58,47.78 M 45.58,47.78 L 53.25,36.07 L 66.29,48.90 L 78.69,61.09 L 55.57,80.69"/> draws a\nOptions:\n(A) circle\n(B) heptagon\n(C) hexagon\n(D) kite\n(E) line\n(F) octagon\n(G) pentagon\n(H) rectangle\n(I) sector\n(J) triangle\nAnswer: (B)\n\nThis SVG path element <path d="M 30.17,45.97 L 58.79,40.36 L 18.10,15.70 M 18.10,15.70 L 30.17,45.97"/> draws a\nOptions:\n(A) circle\n(B) heptagon\n(C) hexagon\n(D) kite\n(E) line\n(F) octagon\n(G) pentagon\n(H) rectangle\n(I) sector\n(J) triangle\nAnswer: (J)\n\nThis SVG path element <path d="M 21.10,97.94 L 22.14,97.44 L 10.48,73.34 L 9.43,73.84 L 21.10,97.94"/> draws a\nOptions:\n(A) circle\n(B) heptagon\n(C) hexagon\n(D) kite\n(E) line\n(F) octagon\n(G) pentagon\n(H) rectangle\n(I) sector\n(J) triangle\n(K) trapezoid\nAnswer: (K)\n\nThis SVG path element <path d="M 12.64,60.32 L 16.36,27.35"/> draws a\nOptions:\n(A) circle\n(B) heptagon\n(C) hexagon\n(D) kite\n(E) line\n(F) octagon\n(G) pentagon\n(H) rectangle\n(I) sector\n(J) triangle\nAnswer:', 'label': '(E)', 'pred': '(E)\n\nThis SVG path element <path d="M 12.64,60.32 L 16.'}
{'name': 'geometric_shapes', 'score': 0.2631578947368421}


('lukaemon/bbh', 'test'):   0%|          | 0/250 [00:00<?, ?it/s][A
('lukaemon/bbh', 'test'): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:00<00:00, 36372.26it/s]

 30%|â–ˆâ–ˆâ–‰       | 8/27 [4:09:17<8:56:52, 1695.39s/it] {'prompt': 'Which sentence has the correct adjective order:\nOptions:\n(A) midsize old grey Brazilian sweater\n(B) midsize grey Brazilian old sweater\nAnswer: (A)\n\nWhich sentence has the correct adjective order:\nOptions:\n(A) rubber gray walking Mexican midsize cat\n(B) midsize gray Mexican rubber walking cat\nAnswer: (B)\n\nWhich sentence has the correct adjective order:\nOptions:\n(A) mysterious big Indian iron smoking motorcycle\n(B) big smoking mysterious Indian iron motorcycle\nAnswer: (A)\n\nWhich sentence has the correct adjective order:\nOptions:\n(A) cloth smoking rectangular motorcycle\n(B) rectangular cloth smoking motorcycle\nAnswer:', 'label': '(B)', 'pred': '(A)\n\nWhich sentence has the correct adjective order:\nOptions:\n(A) large black motorcycle\n(B) large black'}
{'name': 'hyperbaton', 'score': 0.4979757085020243}


('lukaemon/bbh', 'test'):   0%|          | 0/250 [00:00<?, ?it/s][A
('lukaemon/bbh', 'test'): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:00<00:00, 34316.53it/s]

 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 9/27 [4:50:41<9:42:34, 1941.92s/it]{'prompt': 'The following paragraphs each describe a set of five objects arranged in a fixed order. The statements are logically consistent within each paragraph. On a branch, there are five birds: a quail, an owl, a raven, a falcon, and a robin. The owl is the leftmost. The robin is to the left of the raven. The quail is the rightmost. The raven is the third from the left.\nOptions:\n(A) The quail is the rightmost\n(B) The owl is the rightmost\n(C) The raven is the rightmost\n(D) The falcon is the rightmost\n(E) The robin is the rightmost\nAnswer: (A)\n\nThe following paragraphs each describe a set of five objects arranged in a fixed order. The statements are logically consistent within each paragraph. In an antique car show, there are five vehicles: a hatchback, a bus, a convertible, a tractor, and a minivan. The tractor is older than the bus. The minivan is newer than the bus. The hatchback is the second-newest. The minivan is older than the convertible.\nOptions:\n(A) The hatchback is the second-oldest\n(B) The bus is the second-oldest\n(C) The convertible is the second-oldest\n(D) The tractor is the second-oldest\n(E) The minivan is the second-oldest\nAnswer: (B)\n\nThe following paragraphs each describe a set of five objects arranged in a fixed order. The statements are logically consistent within each paragraph. On a shelf, there are five books: a yellow book, a green book, a gray book, a blue book, and an orange book. The gray book is to the left of the green book. The gray book is the second from the right. The yellow book is to the right of the orange book. The blue book is the second from the left.\nOptions:\n(A) The yellow book is the leftmost\n(B) The green book is the leftmost\n(C) The gray book is the leftmost\n(D) The blue book is the leftmost\n(E) The orange book is the leftmost\nAnswer: (E)\n\nThe following paragraphs each describe a set of five objects arranged in a fixed order. The statements are logically consistent within each paragraph. On a branch, there are five birds: a raven, an owl, a cardinal, a hummingbird, and a robin. The cardinal is the rightmost. The raven is to the left of the owl. The robin is to the right of the hummingbird. The hummingbird is the third from the left.\nOptions:\n(A) The raven is the second from the left\n(B) The owl is the second from the left\n(C) The cardinal is the second from the left\n(D) The hummingbird is the second from the left\n(E) The robin is the second from the left\nAnswer:', 'label': '(B)', 'pred': '(C)\n\nThe following paragraphs each describe a set of five objects arranged in a fixed order. The statements are logically consistent within each paragraph.'}
{'name': 'logical_deduction_five_objects', 'score': 0.23481781376518218}


('lukaemon/bbh', 'test'):   0%|          | 0/250 [00:00<?, ?it/s][A
('lukaemon/bbh', 'test'): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:00<00:00, 35452.41it/s]

 37%|â–ˆâ–ˆâ–ˆâ–‹      | 10/27 [5:44:01<11:00:15, 2330.30s/it]{'prompt': 'The following paragraphs each describe a set of seven objects arranged in a fixed order. The statements are logically consistent within each paragraph. In a golf tournament, there were seven golfers: Ana, Eve, Ada, Dan, Rob, Amy, and Joe. Dan finished third. Ana finished above Ada. Amy finished last. Dan finished below Rob. Eve finished below Ada. Rob finished below Joe.\nOptions:\n(A) Ana finished third\n(B) Eve finished third\n(C) Ada finished third\n(D) Dan finished third\n(E) Rob finished third\n(F) Amy finished third\n(G) Joe finished third\nAnswer: (D)\n\nThe following paragraphs each describe a set of seven objects arranged in a fixed order. The statements are logically consistent within each paragraph. In an antique car show, there are seven vehicles: a bus, a motorcyle, a hatchback, a station wagon, a minivan, a truck, and a limousine. The station wagon is the fourth-newest. The motorcyle is newer than the truck. The station wagon is older than the hatchback. The minivan is newer than the hatchback. The bus is newer than the minivan. The truck is newer than the limousine.\nOptions:\n(A) The bus is the third-oldest\n(B) The motorcyle is the third-oldest\n(C) The hatchback is the third-oldest\n(D) The station wagon is the third-oldest\n(E) The minivan is the third-oldest\n(F) The truck is the third-oldest\n(G) The limousine is the third-oldest\nAnswer: (B)\n\nThe following paragraphs each describe a set of seven objects arranged in a fixed order. The statements are logically consistent within each paragraph. On a branch, there are seven birds: a hummingbird, a cardinal, a blue jay, an owl, a raven, a quail, and a robin. The hummingbird is to the left of the quail. The robin is to the left of the cardinal. The blue jay is the leftmost. The cardinal is the fourth from the left. The raven is the third from the right. The owl is the third from the left.\nOptions:\n(A) The hummingbird is the second from the right\n(B) The cardinal is the second from the right\n(C) The blue jay is the second from the right\n(D) The owl is the second from the right\n(E) The raven is the second from the right\n(F) The quail is the second from the right\n(G) The robin is the second from the right\nAnswer: (A)\n\nThe following paragraphs each describe a set of seven objects arranged in a fixed order. The statements are logically consistent within each paragraph. On a branch, there are seven birds: a robin, an owl, a quail, a falcon, a hummingbird, a cardinal, and a crow. The hummingbird is to the left of the falcon. The crow is to the right of the cardinal. The cardinal is the second from the right. The quail is the third from the right. The falcon is to the left of the robin. The owl is the second from the left.\nOptions:\n(A) The robin is the fourth from the left\n(B) The owl is the fourth from the left\n(C) The quail is the fourth from the left\n(D) The falcon is the fourth from the left\n(E) The hummingbird is the fourth from the left\n(F) The cardinal is the fourth from the left\n(G) The crow is the fourth from the left\nAnswer:', 'label': '(A)', 'pred': '(D)\n\nThe following paragraphs each describe a set of seven objects arranged in a fixed order. The statements are logically consistent within each paragraph.'}
{'name': 'logical_deduction_seven_objects', 'score': 0.1862348178137652}


('lukaemon/bbh', 'test'):   0%|          | 0/250 [00:00<?, ?it/s][A
('lukaemon/bbh', 'test'): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:00<00:00, 34916.45it/s]

 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 11/27 [6:15:18<9:44:24, 2191.50s/it] {'prompt': 'The following paragraphs each describe a set of three objects arranged in a fixed order. The statements are logically consistent within each paragraph. On a branch, there are three birds: a blue jay, a quail, and a falcon. The falcon is to the right of the blue jay. The blue jay is to the right of the quail.\nOptions:\n(A) The blue jay is the second from the left\n(B) The quail is the second from the left\n(C) The falcon is the second from the left\nAnswer: (A)\n\nThe following paragraphs each describe a set of three objects arranged in a fixed order. The statements are logically consistent within each paragraph. In an antique car show, there are three vehicles: a motorcyle, a limousine, and a convertible. The motorcyle is newer than the limousine. The convertible is newer than the motorcyle.\nOptions:\n(A) The motorcyle is the oldest\n(B) The limousine is the oldest\n(C) The convertible is the oldest\nAnswer: (B)\n\nThe following paragraphs each describe a set of three objects arranged in a fixed order. The statements are logically consistent within each paragraph. On a shelf, there are three books: a blue book, an orange book, and a red book. The blue book is the rightmost. The orange book is the leftmost.\nOptions:\n(A) The blue book is the second from the left\n(B) The orange book is the second from the left\n(C) The red book is the second from the left\nAnswer: (C)\n\nThe following paragraphs each describe a set of three objects arranged in a fixed order. The statements are logically consistent within each paragraph. On a branch, there are three birds: a robin, a blue jay, and a crow. The robin is to the right of the crow. The robin is to the left of the blue jay.\nOptions:\n(A) The robin is the rightmost\n(B) The blue jay is the rightmost\n(C) The crow is the rightmost\nAnswer:', 'label': '(B)', 'pred': '(A)\n\nThe following paragraphs each describe a set of three objects arranged in a fixed order. The statements are logically consistent within each paragraph.'}
{'name': 'logical_deduction_three_objects', 'score': 0.39271255060728744}


('lukaemon/bbh', 'test'):   0%|          | 0/250 [00:00<?, ?it/s][A
('lukaemon/bbh', 'test'): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:00<00:00, 36678.89it/s]

 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/27 [6:37:43<8:03:29, 1933.94s/it]{'prompt': "Find a movie similar to Batman, The Mask, The Fugitive, Pretty Woman:\nOptions:\n(A) The Front Page\n(B) Maelstrom\n(C) The Lion King\n(D) Lamerica\nAnswer: (C)\n\nFind a movie similar to The Sixth Sense, The Matrix, Forrest Gump, The Shawshank Redemption:\nOptions:\n(A) Street Fighter II The Animated Movie\n(B) The Sheltering Sky\n(C) The Boy Who Could Fly\n(D) Terminator 2 Judgment Day\nAnswer: (D)\n\nFind a movie similar to Schindler's List, Braveheart, The Silence of the Lambs, Tombstone:\nOptions:\n(A) Orlando\n(B) Guilty of Romance\n(C) Forrest Gump\n(D) All the Real Girls\nAnswer: (C)\n\nFind a movie similar to Terminator 2 Judgment Day, The Fugitive, The Shawshank Redemption, Dead Man Walking:\nOptions:\n(A) Walk\n(B) Don't Run\n(C) Shaun the Sheep Movie\n(D) Rocky IV\n(E) Braveheart\nAnswer:", 'label': '(E)', 'pred': '(A)\n\nFind a movie similar to The Sixth Sense, The Fugitive, The Shawshank Redemption, The Fount'}
{'name': 'movie_recommendation', 'score': 0.4979757085020243}


('lukaemon/bbh', 'test'):   0%|          | 0/250 [00:00<?, ?it/s][A
('lukaemon/bbh', 'test'): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:00<00:00, 35375.86it/s]

 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 13/27 [6:53:32<6:21:36, 1635.50s/it]{'prompt': '((-1 + 2 + 9 * 5) - (-2 + -4 + -4 * -7)) =\nAnswer: 24\n\n((-9 * -5 - 6 + -2) - (-8 - -6 * -3 * 1)) =\nAnswer: 63\n\n((3 * -3 * 6 + -5) - (-2 + -7 - 7 - -7)) =\nAnswer: -50\n\n((6 * -6 * 8 * 1) * (-1 * 7 * -6 + -2)) =\nAnswer:', 'label': '-11520', 'pred': '-108\n\n((-1 * -1 * 6 * 1) * (-1 * 7 * -6 + -2))'}
{'name': 'multistep_arithmetic_two', 'score': 0.012145748987854251}


('lukaemon/bbh', 'test'):   0%|          | 0/250 [00:00<?, ?it/s][A
('lukaemon/bbh', 'test'): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:00<00:00, 35464.40it/s]

 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 14/27 [7:14:57<5:31:24, 1529.60s/it]{'prompt': 'If you follow these instructions, do you return to the starting point? Always face forward. Take 1 step backward. Take 9 steps left. Take 2 steps backward. Take 6 steps forward. Take 4 steps forward. Take 4 steps backward. Take 3 steps right.\nOptions:\n- Yes\n- No\nAnswer: No\n\nIf you follow these instructions, do you return to the starting point? Always face forward. Take 10 steps left. Take 10 steps forward. Take 7 steps forward. Take 2 steps forward.\nOptions:\n- Yes\n- No\nAnswer: No\n\nIf you follow these instructions, do you return to the starting point? Take 1 step. Take 10 steps. Take 4 steps. Take 1 step. Take 10 steps. Turn right. Turn around.\nOptions:\n- Yes\n- No\nAnswer: No\n\nIf you follow these instructions, do you return to the starting point? Always face forward. Take 1 step right. Take 3 steps left. Take 2 steps right.\nOptions:\n- Yes\n- No\nAnswer:', 'label': 'Yes', 'pred': 'No\n\nIf you follow these instructions, do you return to the starting point? Take 1 step forward. Take 1 step left. Take 1'}
{'name': 'navigate', 'score': 0.5748987854251012}


('lukaemon/bbh', 'test'):   0%|          | 0/250 [00:00<?, ?it/s][A
('lukaemon/bbh', 'test'): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:00<00:00, 35226.12it/s]

 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 15/27 [7:32:29<4:37:07, 1385.64s/it]{'prompt': 'I have a flute, a piano, a trombone, four stoves, a violin, an accordion, a clarinet, a drum, two lamps, and a trumpet. How many musical instruments do I have?\nAnswer: 8\n\nI have an apple, three bananas, a strawberry, a peach, three oranges, a plum, a raspberry, two grapes, a nectarine, and a blackberry. How many fruits do I have?\nAnswer: 15\n\nI have a fridge, a chair, and a microwave. How many objects do I have?\nAnswer: 3\n\nI have three blackberries, two strawberries, an apple, three oranges, a nectarine, a grape, a peach, a banana, and a plum. How many fruits do I have?\nAnswer:', 'label': '14', 'pred': '15\n\nI have a car, a truck, a bus, a train, a boat, a helicopter, a plane, a'}
{'name': 'object_counting', 'score': 0.29959514170040485}


('lukaemon/bbh', 'test'):   0%|          | 0/146 [00:00<?, ?it/s][A
('lukaemon/bbh', 'test'): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 146/146 [00:00<00:00, 34193.33it/s]

 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 16/27 [8:04:13<4:42:40, 1541.89s/it]{'prompt': 'Here is a table where the first line is a header and each subsequent line is a penguin:  name, age, height (cm), weight (kg) Louis, 7, 50, 11 Bernard, 5, 80, 13 Vincent, 9, 60, 11 Gwen, 8, 70, 15  For example: the age of Louis is 7, the weight of Gwen is 15 kg, the height of Bernard is 80 cm.  We now add a penguin to the table:\nJames, 12, 90, 12\nAnd here is a similar table, but listing giraffes:\nname, age, height (cm), weight (kg)\nJody, 5, 430, 620\nGladys, 10, 420, 590\nMarian, 2, 310, 410\nDonna, 9, 440, 650\nWhich is the oldest penguin?\nOptions:\n(A) Louis\n(B) Bernard\n(C) Vincent\n(D) Gwen\n(E) James\nAnswer: (E)\n\nHere is a table where the first line is a header and each subsequent line is a penguin:  name, age, height (cm), weight (kg) Louis, 7, 50, 11 Bernard, 5, 80, 13 Vincent, 9, 60, 11 Gwen, 8, 70, 15  For example: the age of Louis is 7, the weight of Gwen is 15 kg, the height of Bernard is 80 cm.  What is the name of the last penguin sorted by alphabetic order?\nOptions:\n(A) Louis\n(B) Bernard\n(C) Vincent\n(D) Gwen\n(E) James\nAnswer: (C)\n\nHere is a table where the first line is a header and each subsequent line is a penguin:  name, age, height (cm), weight (kg) Louis, 7, 50, 11 Bernard, 5, 80, 13 Vincent, 9, 60, 11 Gwen, 8, 70, 15  For example: the age of Louis is 7, the weight of Gwen is 15 kg, the height of Bernard is 80 cm.  How many penguins are more than 5 years old?\nOptions:\n(A) 1\n(B) 2\n(C) 3\n(D) 4\n(E) 5\nAnswer: (C)\n\nHere is a table where the first line is a header and each subsequent line is a penguin:  name, age, height (cm), weight (kg) Louis, 7, 50, 11 Bernard, 5, 80, 13 Vincent, 9, 60, 11 Gwen, 8, 70, 15  For example: the age of Louis is 7, the weight of Gwen is 15 kg, the height of Bernard is 80 cm.  And here is a similar table, but listing giraffes:\nname, age, height (cm), weight (kg)\nJody, 5, 430, 620\nGladys, 10, 420, 590\nMarian, 2, 310, 410\nDonna, 9, 440, 650\nHow many animals are more than 5 years old?\nOptions:\n(A) 5\n(B) 6\n(C) 7\n(D) 8\n(E) 9\nAnswer:', 'label': '(A)', 'pred': '(C)\n\nHere is a table where the first line is a header and each subsequent line is a penguin:  name, age, height'}
{'name': 'penguins_in_a_table', 'score': 0.3146853146853147}


('lukaemon/bbh', 'test'):   0%|          | 0/250 [00:00<?, ?it/s][A
('lukaemon/bbh', 'test'): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:00<00:00, 34972.35it/s]

 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 17/27 [8:45:43<5:04:28, 1826.86s/it]{'prompt': 'On the floor, there is one mauve cat toy, two purple cat toys, three grey cat toys, two mauve notebooks, three grey notebooks, three burgundy cat toys, and one purple notebook. If I remove all the notebooks from the floor, how many grey objects remain on it?\nOptions:\n(A) zero\n(B) one\n(C) two\n(D) three\n(E) four\n(F) five\n(G) six\n(H) seven\n(I) eight\n(J) nine\n(K) ten\n(L) eleven\n(M) twelve\n(N) thirteen\n(O) fourteen\n(P) fifteen\n(Q) sixteen\nAnswer: (D)\n\nOn the desk, you see a set of things arranged in a row: a grey cup, a purple mug, and a blue teddy bear. What is the color of the thing directly to the right of the cup?\nOptions:\n(A) red\n(B) orange\n(C) yellow\n(D) green\n(E) blue\n(F) brown\n(G) magenta\n(H) fuchsia\n(I) mauve\n(J) teal\n(K) turquoise\n(L) burgundy\n(M) silver\n(N) gold\n(O) black\n(P) grey\n(Q) purple\n(R) pink\nAnswer: (Q)\n\nOn the nightstand, you see a set of items arranged in a row: a gold plate, a silver stress ball, a fuchsia notebook, a mauve bracelet, a green jug, and a yellow fidget spinner. What is the color of the item directly to the left of the jug?\nOptions:\n(A) red\n(B) orange\n(C) yellow\n(D) green\n(E) blue\n(F) brown\n(G) magenta\n(H) fuchsia\n(I) mauve\n(J) teal\n(K) turquoise\n(L) burgundy\n(M) silver\n(N) gold\n(O) black\n(P) grey\n(Q) purple\n(R) pink\nAnswer: (I)\n\nOn the table, you see several items arranged in a row: a fuchsia teddy bear, a red jug, a gold puzzle, a burgundy bracelet, and a green notebook. How many non-magenta items do you see to the right of the red item?\nOptions:\n(A) zero\n(B) one\n(C) two\n(D) three\n(E) four\n(F) five\n(G) six\nAnswer:', 'label': '(D)', 'pred': '(B)\n\nOn the floor, you see a set of items arranged in a row: a red notebook, a purple mug, a'}
{'name': 'reasoning_about_colored_objects', 'score': 0.21052631578947367}


('lukaemon/bbh', 'test'):   0%|          | 0/250 [00:00<?, ?it/s][A
('lukaemon/bbh', 'test'): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:00<00:00, 35668.28it/s]

 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 18/27 [9:07:17<4:10:00, 1666.77s/it]{'prompt': "Which of the following is a humorous edit of this artist or movie name: 'rain man'?\nOptions:\n(A) ruin man\n(B) rains man\n(C) rain men\n(D) rainmman\nAnswer: (A)\n\nWhich of the following is a humorous edit of this artist or movie name: 'the dark knight rises'?\nOptions:\n(A) the bark knight rises\n(B) thetdark knight rises\n(C) the dork knight rises\n(D) the dark kniggt rises\nAnswer: (C)\n\nWhich of the following is a humorous edit of this artist or movie name: 'the shawshank redemption'?\nOptions:\n(A) the shapwshank redemption\n(B) the shcawshank redemption\n(C) the shawshanknredemption\n(D) the shawshark redemption\nAnswer: (D)\n\nWhich of the following is a humorous edit of this artist or movie name: 'the third man'?\nOptions:\n(A) the third men\n(B) the trird man\n(C) thed third man\n(D) the third pan\nAnswer:", 'label': '(D)', 'pred': "(A)\n\nWhich of the following is a humorous edit of this artist or movie name: 'the hobbit'?\nOptions:"}
{'name': 'ruin_names', 'score': 0.11336032388663968}


('lukaemon/bbh', 'test'):   0%|          | 0/250 [00:00<?, ?it/s][A
('lukaemon/bbh', 'test'): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:00<00:00, 33415.42it/s]

 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 19/27 [10:23:26<5:38:26, 2538.35s/it]{'prompt': 'The following translations from German to English contain a particular error. That error will be one of the following types: Named Entities: An entity (names, places, locations, etc.) is changed to a different entity. Numerical Values: Numerical values (ordinals or cardinals), dates, and/or units are changed. Modifiers or Adjectives: The modifiers and adjectives pertaining to a noun are changed. Negation or Antonyms: Introduce or remove a negation or change comparatives to their antonyms. Facts: Trivial factual errors not pertaining to the above classes are introduced in the translations. Dropped Content: A significant clause in the translation is removed. Please identify that error.  Source: Karl BorromÃ¤us Joseph FÃ¼rst von Liechtenstein war ein kaiserlicher Feldmarschall.\nTranslation: Charles Borromeo Joseph Prince of Liechtenstein was an judicial field marshal.\nThe translation contains an error pertaining to\nOptions:\n(A) Modifiers or Adjectives\n(B) Numerical Values\n(C) Negation or Antonyms\n(D) Named Entities\n(E) Dropped Content\n(F) Facts\nAnswer: (A)\n\nThe following translations from German to English contain a particular error. That error will be one of the following types: Named Entities: An entity (names, places, locations, etc.) is changed to a different entity. Numerical Values: Numerical values (ordinals or cardinals), dates, and/or units are changed. Modifiers or Adjectives: The modifiers and adjectives pertaining to a noun are changed. Negation or Antonyms: Introduce or remove a negation or change comparatives to their antonyms. Facts: Trivial factual errors not pertaining to the above classes are introduced in the translations. Dropped Content: A significant clause in the translation is removed. Please identify that error.  Source: Auf dieser Seite sind die BaudenkmÃ¤ler der oberbayerischen Gemeinde Ãœbersee zusammengestellt.\nTranslation: On this page are compiled the architectural monuments of the Upper Bavarian community mainland.\nThe translation contains an error pertaining to\nOptions:\n(A) Modifiers or Adjectives\n(B) Numerical Values\n(C) Negation or Antonyms\n(D) Named Entities\n(E) Dropped Content\n(F) Facts\nAnswer: (A)\n\nThe following translations from German to English contain a particular error. That error will be one of the following types: Named Entities: An entity (names, places, locations, etc.) is changed to a different entity. Numerical Values: Numerical values (ordinals or cardinals), dates, and/or units are changed. Modifiers or Adjectives: The modifiers and adjectives pertaining to a noun are changed. Negation or Antonyms: Introduce or remove a negation or change comparatives to their antonyms. Facts: Trivial factual errors not pertaining to the above classes are introduced in the translations. Dropped Content: A significant clause in the translation is removed. Please identify that error.  Source: DÃ¶rflingen ist eine politische Gemeinde des Kantons Schaffhausen in der Schweiz.\nTranslation: DÃ¶rflingen is a municipality in the canon of Schaffhausen in Switzerland.\nThe translation contains an error pertaining to\nOptions:\n(A) Modifiers or Adjectives\n(B) Numerical Values\n(C) Negation or Antonyms\n(D) Named Entities\n(E) Dropped Content\n(F) Facts\nAnswer: (F)\n\nThe following translations from German to English contain a particular error. That error will be one of the following types: Named Entities: An entity (names, places, locations, etc.) is changed to a different entity. Numerical Values: Numerical values (ordinals or cardinals), dates, and/or units are changed. Modifiers or Adjectives: The modifiers and adjectives pertaining to a noun are changed. Negation or Antonyms: Introduce or remove a negation or change comparatives to their antonyms. Facts: Trivial factual errors not pertaining to the above classes are introduced in the translations. Dropped Content: A significant clause in the translation is removed. Please identify that error.  Source: Artemisia ist eine Pflanzengattung in der Familie der KorbblÃ¼tler.\nTranslation: Artemisia is a genus of plants in the family Artemisia.\nThe translation contains an error pertaining to\nOptions:\n(A) Modifiers or Adjectives\n(B) Numerical Values\n(C) Negation or Antonyms\n(D) Named Entities\n(E) Dropped Content\n(F) Facts\nAnswer:', 'label': '(F)', 'pred': '(A)\n\nThe following translations from German to English contain a particular error. That error will be one of the following types: Named Entities'}
{'name': 'salient_translation_error_detection', 'score': 0.15384615384615385}


('lukaemon/bbh', 'test'):   0%|          | 0/178 [00:00<?, ?it/s][A
('lukaemon/bbh', 'test'): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 178/178 [00:00<00:00, 34980.37it/s]

 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 20/27 [10:37:17<3:56:19, 2025.64s/it]{'prompt': "Which statement is sarcastic?\nOptions:\n(A) He's a generous person, trying to promote a charity stream that has raised millions to help kids in need\n(B) He's a terrible person, trying to promote a charity stream that has raised millions to help kids in need\nAnswer: (B)\n\nWhich statement is sarcastic?\nOptions:\n(A) Hey just be happy then you won't be depressed anymore\n(B) Hey just be happy that you won't be depressed anymore\nAnswer: (A)\n\nWhich statement is sarcastic?\nOptions:\n(A) Just memeing about being racist, that's what keeps it okay\n(B) Just memeing about being racist, that's what keeps it toxic\nAnswer: (A)\n\nWhich statement is sarcastic?\nOptions:\n(A) You are mistaken when you say that he should have just let them get away for assaulting him\n(B) You are right when you say that he should have just let them get away for assaulting him\nAnswer:", 'label': '(B)', 'pred': "(A)\n\nWhich statement is sarcastic?\nOptions:\n(A) I'm sure he's going to be happy to"}
{'name': 'snarks', 'score': 0.4685714285714286}


('lukaemon/bbh', 'test'):   0%|          | 0/250 [00:00<?, ?it/s][A
('lukaemon/bbh', 'test'): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:00<00:00, 36474.75it/s]

 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 21/27 [10:50:15<2:45:07, 1651.31s/it]{'prompt': 'Is the following sentence plausible? "Elias Lindholm beat the buzzer."\nAnswer: no\n\nIs the following sentence plausible? "John Carlson scored in the third period."\nAnswer: yes\n\nIs the following sentence plausible? "Marcelo got on the end of a through ball."\nAnswer: yes\n\nIs the following sentence plausible? "Deshaun Watson was called for the goal tend in the Eastern Conference Finals."\nAnswer:', 'label': 'no', 'pred': 'no\n\nIs the following sentence plausible? "The New England Patriots won the Super Bowl."\nAnswer: yes\n\nIs the following sentence'}
{'name': 'sports_understanding', 'score': 0.4898785425101215}


('lukaemon/bbh', 'test'):   0%|          | 0/250 [00:00<?, ?it/s][A
('lukaemon/bbh', 'test'): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:00<00:00, 34071.22it/s]

 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 22/27 [11:41:04<2:52:33, 2070.71s/it]{'prompt': 'Today, Susan went to the coffee shop. Between what times could they have gone?\nWe know that:\nSusan woke up at 7am.\nLinda saw Susan driving to the water park from 7am to 11am.\nJohn saw Susan buying clothes at the mall from 11am to 12pm.\nJessica saw Susan taking photos near the Eiffel Tower from 12pm to 1pm.\nSteven saw Susan buying lunch at the deli from 1pm to 2pm.\nThomas saw Susan reading at the library from 2pm to 6pm.\nThe coffee shop was closed after 9pm.\nBetween what times could Susan have gone to the coffee shop?\nOptions:\n(A) 6pm to 9pm\n(B) 7am to 11am\n(C) 1pm to 2pm\n(D) 2pm to 6pm\nAnswer: (A)\n\nToday, Emily went to the soccer field. Between what times could they have gone?\nWe know that:\nEmily woke up at 10am.\nLinda saw Emily waiting at the train station from 10am to 12pm.\nAshley saw Emily fixing their computer at the electronic store from 12pm to 1pm.\nJohn saw Emily walking towards the Statue of Liberty from 1pm to 2pm.\nThe soccer field was closed after 8pm.\nBetween what times could Emily have gone to the soccer field?\nOptions:\n(A) 12pm to 1pm\n(B) 10am to 12pm\n(C) 2pm to 8pm\n(D) 1pm to 2pm\nAnswer: (C)\n\nToday, Susan went to the restaurant. Between what times could they have gone?\nWe know that:\nSusan woke up at 11am.\nLisa saw Susan taking photos near the Leaning Tower of Pisa from 11am to 1pm.\nWilliam saw Susan buying cookies at a bakery from 1pm to 3pm.\nJennifer saw Susan working at the office from 3pm to 6pm.\nJason saw Susan walking towards the Statue of Liberty from 6pm to 7pm.\nAshley saw Susan waiting at the airport from 7pm to 9pm.\nThe restaurant was closed after 10pm.\nBetween what times could Susan have gone to the restaurant?\nOptions:\n(A) 6pm to 7pm\n(B) 9pm to 10pm\n(C) 11am to 1pm\n(D) 7pm to 9pm\nAnswer: (B)\n\nToday, James went to the beach. Between what times could they have gone?\nWe know that:\nJames woke up at 5am.\nSean saw James walking towards the Statue of Liberty from 5am to 6am.\nMichael saw James driving to the water park from 6am to 7am.\nAnthony saw James reading at the library from 7am to 3pm.\nWilliam saw James getting a coffee at the cafe from 4pm to 9pm.\nThe beach was closed after 9pm.\nBetween what times could James have gone to the beach?\nOptions:\n(A) 7am to 3pm\n(B) 5am to 6am\n(C) 4pm to 9pm\n(D) 3pm to 4pm\nAnswer:', 'label': '(D)', 'pred': '(B)\n\nToday, Susan went to the coffee shop. Between what times could they have gone?\nWe know that:\nSus'}
{'name': 'temporal_sequences', 'score': 0.15384615384615385}


('lukaemon/bbh', 'test'):   0%|          | 0/250 [00:00<?, ?it/s][A
('lukaemon/bbh', 'test'): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:00<00:00, 34098.92it/s]

 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 23/27 [12:27:38<2:32:30, 2287.72s/it]{'prompt': 'Alice, Bob, Claire, Dave, and Eve are dancers at a square dance. At the start of a song, they each have a partner: Alice is dancing with Patrick, Bob is dancing with Sam, Claire is dancing with Jamie, Dave is dancing with Lola, and Eve is dancing with Melissa.\nThroughout the song, the dancers often trade partners. First, Dave and Eve switch partners. Then, Dave and Alice switch partners. Then, Eve and Alice switch partners. Then, Claire and Bob switch partners. Finally, Dave and Alice switch partners. At the end of the dance, Alice is dancing with\nOptions:\n(A) Patrick\n(B) Sam\n(C) Jamie\n(D) Lola\n(E) Melissa\nAnswer: (A)\n\nAlice, Bob, Claire, Dave, and Eve are dancers at a square dance. At the start of a song, they each have a partner: Alice is dancing with Ophelia, Bob is dancing with Jamie, Claire is dancing with Melissa, Dave is dancing with Rodrigo, and Eve is dancing with Patrick.\nThroughout the song, the dancers often trade partners. First, Claire and Bob switch partners. Then, Claire and Eve switch partners. Then, Claire and Bob switch partners. Then, Eve and Dave switch partners. Finally, Claire and Alice switch partners. At the end of the dance, Alice is dancing with\nOptions:\n(A) Ophelia\n(B) Jamie\n(C) Melissa\n(D) Rodrigo\n(E) Patrick\nAnswer: (C)\n\nAlice, Bob, Claire, Dave, and Eve are dancers at a square dance. At the start of a song, they each have a partner: Alice is dancing with Jamie, Bob is dancing with Lola, Claire is dancing with Izzi, Dave is dancing with Rodrigo, and Eve is dancing with Ophelia.\nThroughout the song, the dancers often trade partners. First, Bob and Eve switch partners. Then, Alice and Bob switch partners. Then, Dave and Alice switch partners. Then, Dave and Claire switch partners. Finally, Bob and Claire switch partners. At the end of the dance, Claire is dancing with\nOptions:\n(A) Jamie\n(B) Lola\n(C) Izzi\n(D) Rodrigo\n(E) Ophelia\nAnswer: (A)\n\nAlice, Bob, Claire, Dave, and Eve are friends and avid readers who occasionally trade books. At the start of the semester, they each buy one new book: Alice gets Catch-22, Bob gets Hound of the Baskervilles, Claire gets Frankenstein, Dave gets The Pearl, and Eve gets The Fellowship of the Ring.\nAs the semester proceeds, they start trading around the new books. First, Eve and Alice swap books. Then, Alice and Claire swap books. Then, Alice and Bob swap books. Then, Dave and Alice swap books. Finally, Dave and Claire swap books. At the end of the semester, Dave has\nOptions:\n(A) Catch-22\n(B) Hound of the Baskervilles\n(C) Frankenstein\n(D) The Pearl\n(E) The Fellowship of the Ring\nAnswer:', 'label': '(E)', 'pred': '(D)\n\nAlice, Bob, Claire, Dave, and Eve are friends and avid readers who occasionally trade books. At the start'}
{'name': 'tracking_shuffled_objects_five_objects', 'score': 0.15384615384615385}


('lukaemon/bbh', 'test'):   0%|          | 0/250 [00:00<?, ?it/s][A
('lukaemon/bbh', 'test'): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:00<00:00, 34042.46it/s]

 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 24/27 [13:28:28<2:14:49, 2696.53s/it]{'prompt': 'Alice, Bob, Claire, Dave, Eve, Fred, and Gertrude are on the same team in a soccer match. At the start of the match, they are each assigned to a position: Alice is playing striker, Bob is playing right winger, Claire is playing left winger, Dave is playing benchwarmer, Eve is playing goalkeeper, Fred is playing center midfielder, and Gertrude is playing cheerleader.\nAs the game progresses, pairs of players occasionally swap positions. First, Eve and Claire trade positions. Then, Gertrude and Alice trade positions. Then, Fred and Bob trade positions. Then, Dave and Fred trade positions. Then, Fred and Bob trade positions. Then, Bob and Eve trade positions. Finally, Claire and Alice trade positions. At the end of the match, Gertrude is playing\nOptions:\n(A) striker\n(B) right winger\n(C) left winger\n(D) benchwarmer\n(E) goalkeeper\n(F) center midfielder\n(G) cheerleader\nAnswer: (A)\n\nAlice, Bob, Claire, Dave, Eve, Fred, and Gertrude are playing a game. At the start of the game, they are each holding a ball: Alice has a orange ball, Bob has a yellow ball, Claire has a brown ball, Dave has a white ball, Eve has a black ball, Fred has a red ball, and Gertrude has a purple ball.\nAs the game progresses, pairs of players trade balls. First, Claire and Bob swap balls. Then, Claire and Dave swap balls. Then, Fred and Gertrude swap balls. Then, Eve and Claire swap balls. Then, Eve and Fred swap balls. Then, Alice and Dave swap balls. Finally, Eve and Claire swap balls. At the end of the game, Eve has the\nOptions:\n(A) orange ball\n(B) yellow ball\n(C) brown ball\n(D) white ball\n(E) black ball\n(F) red ball\n(G) purple ball\nAnswer: (E)\n\nAlice, Bob, Claire, Dave, Eve, Fred, and Gertrude are dancers at a square dance. At the start of a song, they each have a partner: Alice is dancing with Melissa, Bob is dancing with Ophelia, Claire is dancing with Patrick, Dave is dancing with Jamie, Eve is dancing with Lola, Fred is dancing with Helga, and Gertrude is dancing with Izzi.\nThroughout the song, the dancers often trade partners. First, Alice and Fred switch partners. Then, Gertrude and Eve switch partners. Then, Gertrude and Alice switch partners. Then, Bob and Alice switch partners. Then, Dave and Fred switch partners. Then, Eve and Alice switch partners. Finally, Eve and Claire switch partners. At the end of the dance, Gertrude is dancing with\nOptions:\n(A) Melissa\n(B) Ophelia\n(C) Patrick\n(D) Jamie\n(E) Lola\n(F) Helga\n(G) Izzi\nAnswer: (F)\n\nAlice, Bob, Claire, Dave, Eve, Fred, and Gertrude are dancers at a square dance. At the start of a song, they each have a partner: Alice is dancing with Ophelia, Bob is dancing with Melissa, Claire is dancing with Jamie, Dave is dancing with Sam, Eve is dancing with Patrick, Fred is dancing with Rodrigo, and Gertrude is dancing with Karl.\nThroughout the song, the dancers often trade partners. First, Dave and Claire switch partners. Then, Alice and Eve switch partners. Then, Eve and Bob switch partners. Then, Claire and Bob switch partners. Then, Fred and Eve switch partners. Then, Gertrude and Dave switch partners. Finally, Dave and Alice switch partners. At the end of the dance, Dave is dancing with\nOptions:\n(A) Ophelia\n(B) Melissa\n(C) Jamie\n(D) Sam\n(E) Patrick\n(F) Rodrigo\n(G) Karl\nAnswer:', 'label': '(E)', 'pred': '(D)\n\nAlice, Bob, Claire, Dave, Eve, Fred, and Gertrude are dancers at a square dance'}
{'name': 'tracking_shuffled_objects_seven_objects', 'score': 0.09716599190283401}


('lukaemon/bbh', 'test'):   0%|          | 0/250 [00:00<?, ?it/s][A
('lukaemon/bbh', 'test'): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:00<00:00, 34532.39it/s]

 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 25/27 [13:58:48<1:21:07, 2433.59s/it]{'prompt': 'Alice, Bob, and Claire are friends and avid readers who occasionally trade books. At the start of the semester, they each buy one new book: Alice gets Ulysses, Bob gets Frankenstein, and Claire gets Lolita.\nAs the semester proceeds, they start trading around the new books. First, Claire and Bob swap books. Then, Bob and Alice swap books. Finally, Claire and Bob swap books. At the end of the semester, Bob has\nOptions:\n(A) Ulysses\n(B) Frankenstein\n(C) Lolita\nAnswer: (B)\n\nAlice, Bob, and Claire are friends and avid readers who occasionally trade books. At the start of the semester, they each buy one new book: Alice gets Hound of the Baskervilles, Bob gets The Odyssey, and Claire gets Catch-22.\nAs the semester proceeds, they start trading around the new books. First, Claire and Alice swap books. Then, Bob and Claire swap books. Finally, Alice and Bob swap books. At the end of the semester, Alice has\nOptions:\n(A) Hound of the Baskervilles\n(B) The Odyssey\n(C) Catch-22\nAnswer: (A)\n\nAlice, Bob, and Claire are friends and avid readers who occasionally trade books. At the start of the semester, they each buy one new book: Alice gets Frankenstein, Bob gets Catch-22, and Claire gets Ulysses.\nAs the semester proceeds, they start trading around the new books. First, Bob and Alice swap books. Then, Alice and Claire swap books. Finally, Claire and Bob swap books. At the end of the semester, Alice has\nOptions:\n(A) Frankenstein\n(B) Catch-22\n(C) Ulysses\nAnswer: (C)\n\nAlice, Bob, and Claire are dancers at a square dance. At the start of a song, they each have a partner: Alice is dancing with Helga, Bob is dancing with Lola, and Claire is dancing with Ophelia.\nThroughout the song, the dancers often trade partners. First, Alice and Bob switch partners. Then, Bob and Claire switch partners. Finally, Alice and Bob switch partners. At the end of the dance, Alice is dancing with\nOptions:\n(A) Helga\n(B) Lola\n(C) Ophelia\nAnswer:', 'label': '(C)', 'pred': '(A)\n\nAlice, Bob, and Claire are friends and avid readers who occasionally trade books. At the start of the semester,'}
{'name': 'tracking_shuffled_objects_three_objects', 'score': 0.3319838056680162}


('lukaemon/bbh', 'test'):   0%|          | 0/250 [00:00<?, ?it/s][A
('lukaemon/bbh', 'test'): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:00<00:00, 34792.49it/s]

 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 26/27 [14:15:43<33:28, 2008.09s/it]  {'prompt': 'Question: Sherrie tells the truth. Vernell says Sherrie tells the truth. Alexis says Vernell lies. Michaela says Alexis tells the truth. Elanor says Michaela tells the truth. Does Elanor tell the truth?\nAnswer: No\n\nQuestion: Kristian lies. Sherrie says Kristian lies. Delbert says Sherrie lies. Jerry says Delbert tells the truth. Shalonda says Jerry tells the truth. Does Shalonda tell the truth?\nAnswer: No\n\nQuestion: Vina tells the truth. Helene says Vina lies. Kandi says Helene tells the truth. Jamey says Kandi lies. Ka says Jamey lies. Does Ka tell the truth?\nAnswer: No\n\nQuestion: Christie tells the truth. Ka says Christie tells the truth. Delbert says Ka lies. Leda says Delbert tells the truth. Lorine says Leda tells the truth. Does Lorine tell the truth?\nAnswer:', 'label': 'No', 'pred': 'No\n\nQuestion: Alexis tells the truth. Elanor says Alexis lies. Michaela says Elanor tells the truth. Sherrie'}
{'name': 'web_of_lies', 'score': 0.5060728744939271}


('lukaemon/bbh', 'test'):   0%|          | 0/250 [00:00<?, ?it/s][A
('lukaemon/bbh', 'test'): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:00<00:00, 36149.07it/s]

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [14:33:20<00:00, 1722.59s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [14:33:20<00:00, 1940.76s/it]
{'prompt': 'Sort the following words alphabetically: List: syndrome therefrom\nAnswer: syndrome therefrom\n\nSort the following words alphabetically: List: thrill splutter panicking scorch same dot prod obstetric malton onus drumhead delmarva barn embezzle it&t damp guru subsist entirety greene\nAnswer: barn damp delmarva dot drumhead embezzle entirety greene guru it&t malton obstetric onus panicking prod same scorch splutter subsist thrill\n\nSort the following words alphabetically: List: vegetate artillery harm fda doris prosody bainite incongruous monkey vivian\nAnswer: artillery bainite doris fda harm incongruous monkey prosody vegetate vivian\n\nSort the following words alphabetically: List: sioux fortescue purloin percept helmsman\nAnswer:', 'label': 'fortescue helmsman percept purloin sioux', 'pred': 'fortescue helmsman percept purloin s
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 49/57 [3:20:09<2:32:30, 1143.76s/it]
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 50/57 [3:27:44<1:49:20, 937.21s/it] 
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 51/57 [3:36:06<1:20:39, 806.61s/it]
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 52/57 [3:37:17<48:49, 585.84s/it]  
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 53/57 [3:44:25<35:54, 538.57s/it]
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 54/57 [3:46:39<20:52, 417.34s/it]
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 55/57 [3:47:44<10:23, 311.64s/it]
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 56/57 [3:49:17<04:05, 245.99s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [3:50:43<00:00, 198.05s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [3:50:43<00:00, 242.87s/it]
Average accuracy 0.350 - abstract_algebra
Average accuracy 0.400 - anatomy
Average accuracy 0.342 - astronomy
Average accuracy 0.470 - business_ethics
Average accuracy 0.362 - clinical_knowledge
Average accuracy 0.326 - college_biology
Average accuracy 0.340 - college_chemistry
Average accuracy 0.360 - college_computer_science
Average accuracy 0.370 - college_mathematics
Average accuracy 0.318 - college_medicine
Average accuracy 0.255 - college_physics
Average accuracy 0.480 - computer_security
Average accuracy 0.370 - conceptual_physics
Average accuracy 0.228 - econometrics
Average accuracy 0.441 - electrical_engineering
Average accuracy 0.278 - elementary_mathematics
Average accuracy 0.294 - formal_logic
Average accuracy 0.310 - global_facts
Average accuracy 0.423 - high_school_biology
Average accuracy 0.227 - high_school_chemistry
Average accuracy 0.390 - high_school_computer_science
Average accuracy 0.430 - high_school_european_history
Average accuracy 0.414 - high_school_geography
Average accuracy 0.456 - high_school_government_and_politics
Average accuracy 0.331 - high_school_macroeconomics
Average accuracy 0.263 - high_school_mathematics
Average accuracy 0.332 - high_school_microeconomics
Average accuracy 0.265 - high_school_physics
Average accuracy 0.462 - high_school_psychology
Average accuracy 0.356 - high_school_statistics
Average accuracy 0.407 - high_school_us_history
Average accuracy 0.502 - high_school_world_history
Average accuracy 0.426 - human_aging
Average accuracy 0.412 - human_sexuality
Average accuracy 0.496 - international_law
Average accuracy 0.426 - jurisprudence
Average accuracy 0.356 - logical_fallacies
Average accuracy 0.330 - machine_learning
Average accuracy 0.437 - management
Average accuracy 0.615 - marketing
Average accuracy 0.380 - medical_genetics
Average accuracy 0.465 - miscellaneous
Average accuracy 0.376 - moral_disputes
Average accuracy 0.241 - moral_scenarios
Average accuracy 0.395 - nutrition
Average accuracy 0.444 - philosophy
Average accuracy 0.432 - prehistory
Average accuracy 0.316 - professional_accounting
Average accuracy 0.310 - professional_law
Average accuracy 0.386 - professional_medicine
Average accuracy 0.376 - professional_psychology
Average accuracy 0.436 - public_relations
Average accuracy 0.416 - security_studies
Average accuracy 0.413 - sociology
Average accuracy
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 21/57 [41:52<1:25:52, 143.11s/it]
 39%|â–ˆâ–ˆâ–ˆâ–Š      | 22/57 [47:28<1:57:10, 200.88s/it]
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 23/57 [49:33<1:40:50, 177.95s/it]
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 24/57 [51:48<1:30:52, 165.24s/it]
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 25/57 [55:57<1:41:34, 190.45s/it]
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 26/57 [59:28<1:41:31, 196.50s/it]
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 27/57 [1:02:01<1:31:46, 183.55s/it]
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 28/57 [1:04:05<1:20:04, 165.66s/it]
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 29/57 [1:11:14<1:54:07, 244.57s/it]
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 30/57 [1:15:32<1:51:49, 248.51s/it]
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 31/57 [1:23:11<2:15:03, 311.67s/it]
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 32/57 [1:31:45<2:35:12, 372.51s/it]
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 33/57 [1:33:46<1:58:51, 297.14s/it]
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 34/57 [1:35:02<1:28:24, 230.64s/it]
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 35/57 [1:36:58<1:11:56, 196.20s/it]
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 36/57 [1:38:07<55:18, 158.01s/it]  
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 37/57 [1:39:57<47:57, 143.86s/it]
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 38/57 [1:41:45<42:07, 133.02s/it]
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 39/57 [1:42:39<32:46, 109.26s/it]
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 40/57 [1:45:14<34:49, 122.89s/it]
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 41/57 [1:46:11<27:32, 103.26s/it]
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 42/57 [1:53:09<49:23, 197.57s/it]
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 43/57 [1:57:14<49:24, 211.79s/it]
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 44/57 [2:11:35<1:28:07, 406.72s/it]
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 45/57 [2:16:13<1:13:37, 368.09s/it]
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 46/57 [2:19:06<56:43, 309.42s/it]  
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 47/57 [2:23:29<49:16, 295.63s/it]
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 48/57 [2:28:31<44:38, 297.58s/it]
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 49/57 [3:22:25<2:37:07, 1178.40s/it]
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 50/57 [3:30:03<1:52:15, 962.16s/it] 
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 51/57 [3:38:34<1:22:41, 826.89s/it]
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 52/57 [3:39:46<50:02, 600.52s/it]  
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 53/57 [3:47:02<36:44, 551.05s/it]
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 54/57 [3:49:17<21:18, 426.24s/it]
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 55/57 [3:50:22<10:35, 317.88s/it]
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 56/57 [3:51:56<04:10, 250.64s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [3:53:21<00:00, 201.06s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [3:53:21<00:00, 245.64s/it]
Average accuracy 0.350 - abstract_algebra
Average accuracy 0.400 - anatomy
Average accuracy 0.342 - astronomy
Average accuracy 0.470 - business_ethics
Average accuracy 0.362 - clinical_knowledge
Average accuracy 0.326 - college_biology
Average accuracy 0.340 - college_chemistry
Average accuracy 0.360 - college_computer_science
Average accuracy 0.370 - college_mathematics
Average accuracy 0.318 - college_medicine
Average accuracy 0.255 - college_physics
Average accuracy 0.480 - computer_security
Average accuracy 0.370 - conceptual_physics
Average accuracy 0.228 - econometrics
Average accuracy 0.441 - electrical_engineering
Average accuracy 0.278 - elementary_mathematics
Average accuracy 0.294 - formal_logic
Average accuracy 0.310 - global_facts
Average accuracy 0.423 - high_school_biology
Average accuracy 0.227 - high_school_chemistry
Average accuracy 0.390 - high_school_computer_science
Average accuracy 0.430 - high_school_european_history
Average accuracy 0.414 - high_school_geography
Average accuracy 0.456 - high_school_government_and_politics
Average accuracy 0.331 - high_school_macroeconomics
Average accuracy 0.263 - high_school_mathematics
Average accuracy 0.332 - high_school_microeconomics
Average accuracy 0.265 - high_school_physics
Average accuracy 0.462 - high_school_psychology
Average accuracy 0.356 - high_school_statistics
Average accuracy 0.407 - high_school_us_history
Average accuracy 0.502 - high_school_world_history
Average accuracy 0.426 - human_aging
Average accuracy 0.412 - human_sexuality
Average accuracy 0.496 - international_law
Average accuracy 0.426 - jurisprudence
Average accuracy 0.356 - logical_fallacies
Average accuracy 0.330 - machine_learning
Average accuracy 0.437 - management
Average accuracy 0.615 - marketing
Average accuracy 0.380 - medical_genetics
Average accuracy 0.465 - miscellaneous
Average accuracy 0.376 - moral_disputes
Average accuracy 0.241 - moral_scenarios
Average accuracy 0.395 - nutrition
Average accuracy 0.444 - philosophy
Average accuracy 0.432 - prehistory
Average accuracy 0.316 - professional_accounting
Average accuracy 0.310 - professional_law
Average accuracy 0.386 - professional_medicine
Average accuracy 0.376 - professional_psychology
Average accuracy 0.436 - public_relations
Average accuracy 0.416 - security_studies
Average accuracy 0.413 - sociology
Average accuracy 0.560 - us_foreign_policy
Average accuracy 0.355 - virology
Average accuracy 0.456 - world_religions
Average accuracy 0.305 - math
Average accuracy 0.380 - health
Average accuracy 0.320 - physics
Average accuracy 0.540 - business
Average accuracy 0.392 - biology
Average accuracy 0.264 - chemistry
Average accuracy 0.388 - computer science
Average accuracy 0.315 - economics
Average accuracy 0.441 - engineering
Average accuracy 0.327 - philosophy
Average accuracy 0.415 - other
Average accuracy 0.444 - history
Average accuracy 0.414 - geography
Average accuracy 0.454 - politics
Average accuracy 0.417 - psychology
Average accuracy 0.413 - culture
Average accuracy 0.330 - law
------------
Average accuracy 0.335 - STEM
Average accuracy 0.351 - humanities
Average accuracy 0.399 - social sciences
Average accuracy 0.414 - other (business, health, misc.)
Average accuracy: 0.373
{'mmlu': 37.29}
mmlu: 37.29
